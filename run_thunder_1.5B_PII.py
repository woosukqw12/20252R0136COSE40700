import argparse
import torch
from transformers import (
    AutoTokenizer,
    Trainer, 
    TrainingArguments, 
    DataCollatorForTokenClassification, 
    PreTrainedTokenizerFast, 
    AutoModelForTokenClassification,
    AutoConfig
)
import custom
from datasets import load_dataset, Dataset, load_from_disk
from torch.utils.data import Subset
from seqeval.metrics import precision_score, recall_score
from seqeval.metrics import f1_score as seq_f1_score
from seqeval.metrics import classification_report as seq_classification_report
from sklearn.metrics import f1_score as sk_f1_score, cohen_kappa_score
from sklearn.metrics import classification_report as sk_classification_report
import numpy as np
import re
import pandas as pd 
import glob
import os

# --- ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ê²½ë¡œ ---
model_name = "thunder-research-group/SNU_Thunder-DeID-1.5B"
tokenizer_path = "/mnt/data3/Korean_abstraction/python/SNU_Thunder-DeID-main/tokenizer/default_tokenizers/mecab_bpe_deid_128k"

# --- ë°ì´í„°ì…‹ ê²½ë¡œ ---
train_path = './datasets/pii_ner_3dataset_for_thunder/train'
eval_path = './datasets/pii_ner_3dataset_for_thunder/validation'
test_path = './datasets/pii_ner_3dataset_for_thunder/test'


LABEL2ID = {
    "B-ì´ë¦„": 0, "I-ì´ë¦„": 1,
    "B-í•™êµ": 2, "I-í•™êµ": 3,
    "B-íšŒì‚¬": 4, "I-íšŒì‚¬": 5,
    "B-ì£¼ì†Œ": 6, "I-ì£¼ì†Œ": 7,
    "B-ë²ˆí˜¸": 8, "I-ë²ˆí˜¸": 9,
    "B-URL": 10, "I-URL": 11,
    "B-ê³„ì¢Œë²ˆí˜¸": 12, "I-ê³„ì¢Œë²ˆí˜¸": 13,
    "B-ì€í–‰ëª…": 14, "I-ì€í–‰ëª…": 15,
    "B-ë³´ì•ˆì½”ë“œ": 16, "I-ë³´ì•ˆì½”ë“œ": 17,
    "B-ì´ë©”ì¼": 18, "I-ì´ë©”ì¼": 19,
    "B-ì•„ì´ë””": 20, "I-ì•„ì´ë””": 21,
    "O": 22
}
ID2LABEL = {v: k for k, v in LABEL2ID.items()}
num_labels = len(LABEL2ID) # 23


def compute_metric(p):
    preds, labels = p
    preds = np.argmax(preds, axis=2)
    # seqeval ê³„ì‚°ì„ ìœ„í•œ ë¦¬ìŠ¤íŠ¸ (ì—”í‹°í‹° ë‹¨ìœ„)
    true_seqs, pred_seqs = [], []
    true_seqs_binary, pred_seqs_binary = [], [] 
    
    # scikit-learn ê³„ì‚°ì„ ìœ„í•œ 1ì°¨ì› ë¦¬ìŠ¤íŠ¸ (í† í° ë‹¨ìœ„)
    y_true_flat, y_pred_flat = [], []
    y_true_merged, y_pred_merged = [], []
    y_true_binary_flat, y_pred_binary_flat = [], []
    
    for pred_row, label_row in zip(preds, labels):
        true_seq_current, pred_seq_current = [], []
        true_seq_binary_current, pred_seq_binary_current = [], []
        
        for p_id, l_id in zip(pred_row, label_row):
            if l_id == -100:  # padding ë¬´ì‹œ
                continue

            # ğŸ‘ˆ (ìˆ˜ì •) ì „ì—­ ë³€ìˆ˜ ID2LABELì„ ì‚¬ìš©í•˜ë„ë¡ ë³€ê²½
            true_tag = ID2LABEL.get(int(l_id), "O")
            pred_tag = ID2LABEL.get(int(p_id), "O")
            
            # 1. seqevalìš© ë°ì´í„° ì¶”ê°€ (ì—”í‹°í‹° ë‹¨ìœ„)
            true_seq_current.append(true_tag)
            pred_seq_current.append(pred_tag)

            # 2. scikit-learnìš© ë°ì´í„° ì¶”ê°€ (í† í° ë‹¨ìœ„)
            y_true_flat.append(true_tag)
            y_pred_flat.append(pred_tag)
            y_true_merged.append(true_tag[2:] if true_tag != 'O' else 'O')
            y_pred_merged.append(pred_tag[2:] if pred_tag != 'O' else 'O')
            
            y_true_binary_flat.append("O" if true_tag == "O" else "PII")
            y_pred_binary_flat.append("O" if pred_tag == "O" else "PII")
            true_seq_binary_current.append("O" if true_tag == "O" else "PII")
            pred_seq_binary_current.append("O" if pred_tag == "O" else "PII")

        true_seqs.append(true_seq_current)
        pred_seqs.append(pred_seq_current)
        true_seqs_binary.append(true_seq_binary_current)
        pred_seqs_binary.append(pred_seq_binary_current)

    # ë‹¤ì¤‘ í´ë˜ìŠ¤ F1 (ì—”í‹°í‹° ë‹¨ìœ„, seqeval)
    multiclass_micro_f1 = seq_f1_score(true_seqs, pred_seqs, average="micro", zero_division=0)
    multiclass_weighted_f1 = seq_f1_score(true_seqs, pred_seqs, average="weighted", zero_division=0)
    entity_level_binary_f1 = seq_f1_score(true_seqs_binary, pred_seqs_binary, average="micro", zero_division=0)
    report_str = seq_classification_report(true_seqs, pred_seqs, digits=4, zero_division=0)
    
    # í† í° ë‹¨ìœ„ ë©”íŠ¸ë¦­ ê³„ì‚° (sklearn)
    binary_f1 = sk_f1_score(y_true_binary_flat, y_pred_binary_flat, average="weighted", zero_division=0)
    all_bio_labels = list(LABEL2ID.keys())
    all_merged_labels = ['O'] # 'O'ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì¶”ê°€
    for label in all_bio_labels:
        if label != 'O' and label[2:] not in all_merged_labels:
            all_merged_labels.append(label[2:])
            
    token_merged_report_str = sk_classification_report(y_true_merged, y_pred_merged, labels=all_merged_labels, digits=4, zero_division=0)

    tokenlevel_micro_f1 = sk_f1_score(y_true_flat, y_pred_flat, average="micro", zero_division=0)
    kappa = cohen_kappa_score(y_true_flat, y_pred_flat)

    # 4. MUC-style ë¶„ì„ (seqeval ë¦¬í¬íŠ¸ ì¬ê°€ê³µ)
    report_dict = seq_classification_report(true_seqs, pred_seqs, output_dict=True, zero_division=0)
    compact_muc_report = {
        "label": [], 
        "Correct": [], 
        "Spurious": [], 
        "Missing": []
    }
    total_correct, total_spurious, total_missing = 0, 0, 0

    for label, metrics in report_dict.items():
        if label not in ["micro avg", "macro avg", "weighted avg"]:
            support = metrics.get('support', 0)
            precision = metrics.get('precision', 0)
            recall = metrics.get('recall', 0)
            
            correct = int(round(recall * support))
            # Spurious (FP) ê³„ì‚° ìˆ˜ì •: (Correct / Precision) - Correct
            spurious = int(round(correct / precision - correct)) if precision > 0 else (0 if correct == 0 else support)
            missing = support - correct
            
            compact_muc_report["label"].append(label)
            compact_muc_report["Correct"].append(correct)
            compact_muc_report["Spurious"].append(spurious)
            compact_muc_report["Missing"].append(missing)
            
            total_correct += correct
            total_spurious += spurious
            total_missing += missing
            
    # ì „ì²´ ê²°ê³¼(OVERALL) ì¶”ê°€
    compact_muc_report["label"].append("OVERALL")
    compact_muc_report["Correct"].append(total_correct)
    compact_muc_report["Spurious"].append(total_spurious)
    compact_muc_report["Missing"].append(total_missing)
    
    return {
        "entity_level_micro_f1": multiclass_micro_f1,
        "entity_level_weighted_f1": multiclass_weighted_f1,
        "entity_level_binary_f1": entity_level_binary_f1,
        "token_level_micro_f1": tokenlevel_micro_f1,
        "token_level_binary_f1": binary_f1,
        "cohen_kappa": kappa,
        "muc_report": compact_muc_report,
        "entity_level_report": report_str,
        "token_level_report": token_merged_report_str
    }

# -----------------------------------------------------------------
# 3. ë©”ì¸ ì‹¤í–‰ ë¡œì§
# -----------------------------------------------------------------
if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument("--mode", type=str, default="train", help="train/test/infer")
    parser.add_argument("--save_dir", type=str, default="./results/finetuned_thunder_deid")
    parser.add_argument("--dataset_path", type=str, default="./datasets")
    
    args = parser.parse_args()
    
    # ë°ì´í„°ì…‹ ê²½ë¡œ ì„¤ì •
    if args.dataset_path == "./datasets":
        train_path = './datasets/pii_ner_3dataset_for_thunder/train'
        eval_path = './datasets/pii_ner_3dataset_for_thunder/validation'
        test_path = './datasets/pii_ner_3dataset_for_thunder/test'
    else:
        train_path = args.dataset_path + '/train'
        eval_path = args.dataset_path + '/validation'
        test_path = args.dataset_path + '/test'

    # ë°ì´í„°ì…‹ ë¡œë“œ
    print("Loading datasets...")
    train_dataset = load_from_disk(train_path)
    eval_dataset = load_from_disk(eval_path)
    test_dataset = load_from_disk(test_path)
    print("Datasets loaded.")
    
    # í† í¬ë‚˜ì´ì € ë¡œë“œ
    print("Loading tokenizer...")
    tokenizer = PreTrainedTokenizerFast.from_pretrained(tokenizer_path)
    # tokenizer = custom.switch_dummy(tokenizer)
    print("Tokenizer loaded.")
    
    max_len = 512
    data_collator = DataCollatorForTokenClassification(
        tokenizer, 
        padding='max_length', 
        max_length=max_len, 
        label_pad_token_id=-100
    )

    # --------------------------------------------------
    # í•™ìŠµ ëª¨ë“œ
    # --------------------------------------------------
    if args.mode == "train":
        print("Starting [TRAIN] mode...")
        
        print("Dataset already uses 23 labels. Skipping relabeling.")

        # 2. (í•µì‹¬) ëª¨ë¸ Config ìˆ˜ì • ë° ëª¨ë¸ ë¡œë“œ
        print(f"Loading base model '{model_name}' and replacing head for {num_labels} labels.")
        
        config = AutoConfig.from_pretrained(
            model_name,
            num_labels=num_labels,
            id2label=ID2LABEL,
            label2id=LABEL2ID,
            trust_remote_code=True,
        )

        model = AutoModelForTokenClassification.from_pretrained(
            model_name,
            config=config,
            ignore_mismatched_sizes=True,
            trust_remote_code=True,
        )
        print("Model loaded with new classification head.")

        # 3. TrainingArguments ì„¤ì •
        training_args = TrainingArguments(
            output_dir=args.save_dir,
            eval_strategy="epoch",
            save_strategy="epoch",
            learning_rate=3e-5,
            per_device_train_batch_size=2,
            per_device_eval_batch_size=2,
            num_train_epochs=3,
            weight_decay=0.01,
            load_best_model_at_end=True,
            metric_for_best_model="eval_entity_level_micro_f1", 
            bf16=True, 
            report_to="tensorboard",
            logging_steps=100,
        )
        # train_dataset = Subset(train_dataset, list(range(100)))
        # eval_dataset = Subset(eval_dataset, list(range(100)))
        # 4. Trainer ìƒì„±
        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=eval_dataset,
            tokenizer=tokenizer,
            data_collator=data_collator,
            compute_metrics=compute_metric, 
        )

        # 5. í•™ìŠµ ì‹œì‘
        print("Starting training...")
        trainer.train()
        print("Training finished.")

        # 6. ìµœì¢… ëª¨ë¸ ì €ì¥
        print(f"Saving best model to {args.save_dir}")
        trainer.save_model(args.save_dir)
        tokenizer.save_pretrained(args.save_dir)
        print("Model saved.")

    # --------------------------------------------------
    # í…ŒìŠ¤íŠ¸ ëª¨ë“œ
    # --------------------------------------------------
    elif args.mode == "test":
        print("Starting [TEST] mode...")
        torch.cuda.empty_cache()
        ckpt_path = '/mnt/data3/Korean_abstraction/python/coreference/results/finetuned_thunder_deid_1.5B/checkpoint-573300'

        print(f"Loading fine-tuned model from: {ckpt_path}")
        config = AutoConfig.from_pretrained(
            ckpt_path,
            trust_remote_code=True  
        )
        model = AutoModelForTokenClassification.from_pretrained(
            ckpt_path,
            config=config,
            torch_dtype=torch.bfloat16,
            trust_remote_code=True,
        ).to("cuda")
        print("Model loaded.")
        
        print("Test dataset already uses 23 labels. Skipping relabeling.")

        # 3. Trainer ìƒì„± í›„ í‰ê°€
        test_args = TrainingArguments(
            output_dir=args.save_dir + '/test_results',
            per_device_eval_batch_size=8, 
            bf16=True,
            dataloader_drop_last=False,
            report_to="none",
        )

        trainer = Trainer(
            model=model,
            args=test_args,
            data_collator=data_collator,
            tokenizer=tokenizer,
            compute_metrics=compute_metric, 
        )
        
        print("Evaluating test dataset...")
        metrics = trainer.evaluate(test_dataset)
        
        # ğŸ‘ˆ (ìˆ˜ì •) ìƒì„¸í•œ ë©”íŠ¸ë¦­ ì¶œë ¥
        print("\n--- Test Results ---")
        print(f"Entity Level Micro F1      : {metrics.get('eval_entity_level_micro_f1', 0.0):.4f}")
        print(f"Entity Level Weighted F1   : {metrics.get('eval_entity_level_weighted_f1', 0.0):.4f}")
        print(f"Entity Level Binary F1 (PII/O) : {metrics.get('eval_entity_level_binary_f1', 0.0):.4f}")
        print(f"Token Level Micro F1       : {metrics.get('eval_token_level_micro_f1', 0.0):.4f}")
        print(f"Token Level Binary F1 (PII/O)  : {metrics.get('eval_token_level_binary_f1', 0.0):.4f}")
        print(f"Token Level Cohen's Kappa  : {metrics.get('eval_cohen_kappa', 0.0):.4f}")

        print("\n--- MUC-style Report (Correct, Spurious, Missing) ---")
        muc_report_data = metrics.get('eval_muc_report')
        if muc_report_data:
            df = pd.DataFrame(muc_report_data)
            print(df.to_string(index=False))
        else:
            print("MUC Report not found.")

        print("\n--- Entity Level Classification Report (seqeval) ---")
        print(metrics.get('eval_entity_level_report', 'Report not found.'))
        
        print("\n--- Token Level (Merged) Classification Report (sklearn) ---")
        print(metrics.get('eval_token_level_report', 'Report not found.'))