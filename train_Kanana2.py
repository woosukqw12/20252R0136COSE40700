import os
import json
import torch
import argparse
import numpy as np
from datasets import load_dataset, Dataset
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, BitsAndBytesConfig, TrainerCallback, Trainer
from transformers.trainer_utils import EvalLoopOutput
from peft import LoraConfig, get_peft_model, PeftModel, prepare_model_for_kbit_training
from trl import SFTTrainer, SFTConfig
import gc
from tqdm import tqdm
from torch.utils.data import Subset
from datasets import load_from_disk
from sklearn.metrics import f1_score as sk_f1_score
from sklearn.metrics import classification_report as sk_classification_report
from seqeval.metrics import f1_score as seq_f1_score
from seqeval.metrics import classification_report as seq_classification_report
# --- 1. í‰ê°€ í•¨ìˆ˜ ì •ì˜ ---

class CustomStreamTrainer(SFTTrainer):
    def evaluation_loop(
        self,
        dataloader,
        description: str,
        prediction_loss_only: bool | None = None,
        ignore_keys: list[str] | None = None,
        metric_key_prefix: str = "eval",
    ) -> EvalLoopOutput:
        """
        ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ í‰ê°€ ë£¨í”„ë¡œ ì¬ì •ì˜í•©ë‹ˆë‹¤.
        logitsë¥¼ ëˆ„ì í•˜ëŠ” ëŒ€ì‹ , ë°°ì¹˜ë§ˆë‹¤ ë©”íŠ¸ë¦­ì„ ê³„ì‚°í•˜ê³  ì¤‘ê°„ê°’ë§Œ ëˆ„ì í•©ë‹ˆë‹¤.
        """
        model = self._wrap_model(self.model, training=False, dataloader=dataloader)
        model.eval()

        total_tp, total_fp, total_fn = 0, 0, 0
        total_eval_loss = 0.0
        num_eval_samples = 0

        for step, inputs in tqdm(enumerate(dataloader)):
            # --- ğŸ‘‡ í•µì‹¬ ìˆ˜ì • ë¶€ë¶„ ---
            # prediction_stepì˜ ë°˜í™˜ê°’ì— ì˜ì¡´í•˜ì§€ ì•Šê³ , inputsì—ì„œ ì§ì ‘ labelsë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
            # SFTTrainerì˜ ë°ì´í„° ì½œë ˆì´í„°ëŠ” 'labels' í‚¤ë¥¼ ë§Œë“¤ì–´ì£¼ë¯€ë¡œ ì´ í‚¤ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
            labels = inputs.get("labels")
            if labels is None:
                # ë§Œì•½ì„ ìœ„í•œ ëŒ€ë¹„ì±…: labels í‚¤ê°€ ì—†ë‹¤ë©´ input_idsë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
                labels = inputs.get("input_ids")
            # --- ğŸ‘† ---

            with torch.no_grad():
                # ì´ì œ prediction_stepì—ì„œ ë°˜í™˜ë˜ëŠ” labelsëŠ” ì‚¬ìš©í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ _ë¡œ ë°›ìŠµë‹ˆë‹¤.
                loss, logits, _ = self.prediction_step(
                    model, inputs, prediction_loss_only=False, ignore_keys=ignore_keys
                )
            
            # ì§ì ‘ ê°€ì ¸ì˜¨ 'labels' ë³€ìˆ˜ë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ len() ì˜¤ë¥˜ê°€ ë°œìƒí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
            total_eval_loss += loss.item() * len(labels)
            num_eval_samples += len(labels)

            # --- (ì´í›„ ë¡œì§ì€ ë™ì¼) ---
            pred_ids = torch.argmax(logits, axis=-1)
            
            labels = labels.cpu().numpy()
            pred_ids = pred_ids.cpu().numpy()
            
            batch_tp, batch_fp, batch_fn = self._calculate_batch_metrics(labels, pred_ids)
            total_tp += batch_tp
            total_fp += batch_fp
            total_fn += batch_fn
            
            del loss, logits, labels, pred_ids
            gc.collect()

        # ìµœì¢… ë©”íŠ¸ë¦­ ê³„ì‚°
        precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0
        recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0
        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0

        metrics = {
            f"{metric_key_prefix}_loss": total_eval_loss / num_eval_samples if num_eval_samples > 0 else 0,
            f"{metric_key_prefix}_f1_score": f1,
        }

        return EvalLoopOutput(predictions=None, label_ids=None, metrics=metrics, num_samples=num_eval_samples)

    def _calculate_batch_metrics(self, labels, pred_ids):
        """
        ë‹¨ì¼ ë°°ì¹˜ì— ëŒ€í•´ TP, FP, FNì„ ê³„ì‚°í•˜ëŠ” í—¬í¼ í•¨ìˆ˜ì…ë‹ˆë‹¤.
        """
        labels[labels == -100] = self.processing_class.pad_token_id
        batch_tp, batch_fp, batch_fn = 0, 0, 0

        for i in range(pred_ids.shape[0]):
            true_text = self.processing_class.decode(labels[i], skip_special_tokens=True)
            pred_text = self.processing_class.decode(pred_ids[i], skip_special_tokens=True)

            true_entities, pred_entities = [], []
            try: # ì •ë‹µ íŒŒì‹±
                true_json_str = true_text.split('### ë‹µë³€:\n')[-1]
                true_entities = json.loads(true_json_str)
            except (json.JSONDecodeError, IndexError): pass
            
            try: # ì˜ˆì¸¡ íŒŒì‹±
                pred_json_str = pred_text.split('### ë‹µë³€:')[-1].strip()
                if pred_json_str.startswith("```json"): pred_json_str = pred_json_str[7:-3].strip()
                pred_entities = json.loads(pred_json_str)
            except (json.JSONDecodeError, IndexError): pass

            true_set = {json.dumps(e, sort_keys=True) for e in true_entities}
            pred_set = {json.dumps(e, sort_keys=True) for e in pred_entities}
            
            batch_tp += len(true_set.intersection(pred_set))
            batch_fp += len(pred_set - true_set)
            batch_fn += len(true_set - pred_set)
            
        return batch_tp, batch_fp, batch_fn

NORMALIZATION_MAP = {
    'name': 'ì´ë¦„',
    'school': 'í•™êµ',
    'company': 'íšŒì‚¬',
    'organization': 'íšŒì‚¬', # 'organization'ë„ 'íšŒì‚¬'ë¡œ ì²˜ë¦¬
    'address': 'ì£¼ì†Œ',
    'phone': 'ë²ˆí˜¸',
    'number': 'ë²ˆí˜¸',
    'url': 'URL',
    'account_number': 'ê³„ì¢Œë²ˆí˜¸',
    'account': 'ê³„ì¢Œë²ˆí˜¸',
    'bank': 'ì€í–‰ëª…',
    'security_code': 'ë³´ì•ˆì½”ë“œ',
    'email': 'ì´ë©”ì¼',
    'id': 'ì•„ì´ë””',
    'user_id': 'ì•„ì´ë””',
    'username': 'ì•„ì´ë””',
}

def normalize_pii_types(entity_list):
    """
    ì˜ˆì¸¡ëœ ì—”í‹°í‹° ë¦¬ìŠ¤íŠ¸ë¥¼ ë°›ì•„ PII íƒ€ì…ì˜ ê°’ì„ ì •ê·œí™”í•˜ê³ ,
    'type' í‚¤ë¥¼ 'label' í‚¤ë¡œ ë³€ê²½í•©ë‹ˆë‹¤.
    """
    if not isinstance(entity_list, list):
        return []

    normalized_list = []
    for entity in entity_list:
        # ì—”í‹°í‹°ê°€ ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹Œ ê²½ìš°ë¥¼ ëŒ€ë¹„
        if not isinstance(entity, dict): continue
            
        new_entity = entity.copy()
        
        # 'type' í‚¤ê°€ ìˆëŠ”ì§€ ë¨¼ì € í™•ì¸
        if "type" in new_entity:
            # 1. 'type' í‚¤ì˜ ê°’ì„ ê°€ì ¸ì™€ ì†Œë¬¸ìë¡œ ë³€í™˜í•©ë‹ˆë‹¤ (ì˜ˆ: 'Name' -> 'name')
            original_type_value = new_entity.get("type", "").lower()
            
            # 2. NORMALIZATION_MAPì„ ì‚¬ìš©í•´ ê°’ì„ ì •ê·œí™”í•©ë‹ˆë‹¤ (ì˜ˆ: 'name' -> 'ì´ë¦„')
            # ë§µì— ì—†ëŠ” ê°’ì´ë©´ ì›ë˜ ê°’ì„ ìœ ì§€í•©ë‹ˆë‹¤.
            normalized_type_value = NORMALIZATION_MAP.get(original_type_value, new_entity.get("type"))
            
            # 3. 'label'ì´ë¼ëŠ” ìƒˆë¡œìš´ í‚¤ì— ì •ê·œí™”ëœ ê°’ì„ í• ë‹¹í•©ë‹ˆë‹¤.
            new_entity["label"] = normalized_type_value
            
            # 4. ê¸°ì¡´ì˜ 'type' í‚¤ëŠ” ì‚­ì œí•©ë‹ˆë‹¤.
            del new_entity["type"]
        elif "label" in new_entity:
            # 'label' í‚¤ê°€ ì´ë¯¸ ìˆëŠ” ê²½ìš°ì—ë„ ê°’ì„ ì •ê·œí™”í•©ë‹ˆë‹¤.
            original_label_value = new_entity.get("label", "").lower()
            normalized_label_value = NORMALIZATION_MAP.get(original_label_value, new_entity.get("label"))
            new_entity["label"] = normalized_label_value

        normalized_list.append(new_entity)
        
    return normalized_list

def evaluate_predictions(true_entities_list, pred_entities_list):
    """
    ì˜ˆì¸¡ëœ PII(JSON)ì™€ ì‹¤ì œ PII(JSON)ë¥¼ ë¹„êµí•˜ì—¬
    ê°œì²´(Entity) ë‹¨ìœ„ì˜ Precision, Recall, F1-scoreë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    """
    total_tp, total_fp, total_fn = 0, 0, 0

    for true_entities, pred_entities in zip(true_entities_list, pred_entities_list):
        # JSON ê°ì²´ë¥¼ ì •ë ¬ëœ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ ë¹„êµì˜ ì¼ê´€ì„±ì„ ë³´ì¥í•©ë‹ˆë‹¤.
        true_set = {json.dumps(e, sort_keys=True) for e in true_entities}
        pred_set = {json.dumps(e, sort_keys=True) for e in pred_entities}

        tp = len(true_set.intersection(pred_set)) # ì •ë‹µê³¼ ì˜ˆì¸¡ì´ ëª¨ë‘ ì¼ì¹˜
        fp = len(pred_set - true_set)             # ì˜ˆì¸¡ì€ í–ˆì§€ë§Œ ì •ë‹µì—ëŠ” ì—†ìŒ
        fn = len(true_set - pred_set)             # ì •ë‹µì´ì§€ë§Œ ì˜ˆì¸¡í•˜ì§€ ëª»í•¨
        
        total_tp += tp
        total_fp += fp
        total_fn += fn

    # F1 ìŠ¤ì½”ì–´ ê³„ì‚°
    precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0
    recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0
    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0

    return {"precision": precision, "recall": recall, "f1-score": f1}


def compute_metrics(eval_tuple):
    """
    Trainerì˜ ê²€ì¦ ë‹¨ê³„ì—ì„œ í˜¸ì¶œë  í•¨ìˆ˜ì…ë‹ˆë‹¤.
    ëª¨ë¸ì˜ ì˜ˆì¸¡(logits)ì„ ë””ì½”ë”©í•˜ê³  JSONìœ¼ë¡œ íŒŒì‹±í•˜ì—¬ F1-scoreë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    """
    predictions, labels = eval_tuple
    
    # ì˜ˆì¸¡ ê²°ê³¼ì—ì„œ ê°€ì¥ í™•ë¥ ì´ ë†’ì€ í† í° IDë¥¼ ì„ íƒí•©ë‹ˆë‹¤.
    pred_ids = np.argmax(predictions[0], axis=-1)
    del predictions
    gc.collect()
    # ë ˆì´ë¸”ì—ì„œ íŒ¨ë”© í† í°(-100)ì„ tokenizerì˜ pad_token_idë¡œ ë³€ê²½í•©ë‹ˆë‹¤.
    labels[labels == -100] = tokenizer.pad_token_id
    
    # 2. ì¤‘ê°„ ë¦¬ìŠ¤íŠ¸ ìƒì„±ì„ ìµœì†Œí™”í•˜ê³  ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ F1 ìŠ¤ì½”ì–´ì˜ êµ¬ì„± ìš”ì†Œ(tp, fp, fn)ë¥¼ ì§ì ‘ ëˆ„ì í•©ë‹ˆë‹¤.
    total_tp, total_fp, total_fn = 0, 0, 0
    
    # ë°ì´í„°ë¥¼ í•˜ë‚˜ì”© ìˆœíšŒ
    for i in range(pred_ids.shape[0]):
        # ê°œë³„ ìƒ˜í”Œì— ëŒ€í•´ ë””ì½”ë”©
        true_text = tokenizer.decode(labels[i], skip_special_tokens=True)
        pred_text = tokenizer.decode(pred_ids[i], skip_special_tokens=True)
        
        true_entities = []
        pred_entities = []

        # ì •ë‹µ JSON íŒŒì‹±
        try:
            true_json_str = true_text.split('### ë‹µë³€:\n')[-1]
            true_entities = json.loads(true_json_str)
        except (json.JSONDecodeError, IndexError):
            pass # ì‹¤íŒ¨ ì‹œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ìœ ì§€
        
        # ì˜ˆì¸¡ JSON íŒŒì‹±
        try:
            pred_json_str = pred_text.split('### ë‹µë³€:')[-1].strip()
            if pred_json_str.startswith("```json"):
                pred_json_str = pred_json_str[7:-3].strip()
            pred_entities = json.loads(pred_json_str)
        except (json.JSONDecodeError, IndexError):
            pass # ì‹¤íŒ¨ ì‹œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ìœ ì§€

        # ê°œë³„ ìƒ˜í”Œì— ëŒ€í•œ tp, fp, fn ê³„ì‚°
        true_set = {json.dumps(e, sort_keys=True) for e in true_entities}
        pred_set = {json.dumps(e, sort_keys=True) for e in pred_entities}
        
        total_tp += len(true_set.intersection(pred_set))
        total_fp += len(pred_set - true_set)
        total_fn += len(true_set - pred_set)

    # ëˆ„ì ëœ ê°’ìœ¼ë¡œ ìµœì¢… F1 ìŠ¤ì½”ì–´ ê³„ì‚°
    precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0
    recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0
    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
    
    return {"eval_f1_score": f1}

def extract_json_from_text(text):
    # JSON ê°ì²´ëŠ” '{'ë¡œ, JSON ë°°ì—´ì€ '['ë¡œ ì‹œì‘í•©ë‹ˆë‹¤.
    first_brace = text.find('{')
    first_bracket = text.find('[')

    # ì¤‘ê´„í˜¸ë‚˜ ëŒ€ê´„í˜¸ê°€ ì „í˜€ ì—†ìœ¼ë©´ JSONì´ ì—†ëŠ” ê²ƒìœ¼ë¡œ ê°„ì£¼
    if first_brace == -1 and first_bracket == -1:
        return None
    
    # ë” ë¨¼ì € ë‚˜ì˜¤ëŠ” ê²ƒìœ¼ë¡œ ì‹œì‘ì ì„ ì •í•¨
    if first_brace != -1 and (first_bracket == -1 or first_brace < first_bracket):
        start_index = first_brace
    else:
        start_index = first_bracket

    # ì‹œì‘ì ë¶€í„° ë¬¸ìì—´ì„ ì˜ë¼ë‚´ì–´ JSONDecoderë¡œ íŒŒì‹± ì‹œë„
    text_to_decode = text[start_index:]
    try:
        # raw_decodeëŠ” ì²« ë²ˆì§¸ ìœ íš¨í•œ JSON ê°ì²´ë§Œ íŒŒì‹±í•˜ê³ , ë‚˜ë¨¸ì§€ í…ìŠ¤íŠ¸ëŠ” ë¬´ì‹œí•©ë‹ˆë‹¤.
        decoded_json, _ = json.JSONDecoder().raw_decode(text_to_decode)
        return decoded_json
    except json.JSONDecodeError:
        return None

def convert_json_to_bio_tags(text, entities, tokenizer):
    """
    ì›ë³¸ í…ìŠ¤íŠ¸ì™€ PII ì—”í‹°í‹°(JSON)ë¥¼ ë°›ì•„ í† í° ë‹¨ìœ„ BIO íƒœê·¸ ì‹œí€€ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    # 1. ì›ë³¸ í…ìŠ¤íŠ¸ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë¬¸ì ë‹¨ìœ„ íƒœê·¸ ë°°ì—´ ìƒì„±
    
    char_tags = ['O'] * len(text)
    for entity in entities:
        if not isinstance(entity, dict):
            # print("Not dict")
            continue

        label = entity.get("label") or entity.get("type")
        # start = int(entity.get("start"))
        # end = int(entity.get("end"))
        start_val = entity.get("start")
        end_val = entity.get("end")

        # 3. í•„ìˆ˜ ê°’ë“¤ì´ ëª¨ë‘ ì¡´ì¬í•˜ëŠ”ì§€, labelì´ ë¬¸ìì—´ì¸ì§€ í™•ì¸
        if not (label and isinstance(label, str) and start_val is not None and end_val is not None):
            continue

        # 4. start, endë¥¼ ì •ìˆ˜í˜•ìœ¼ë¡œ ë³€í™˜ (íƒ€ì…ì´ int ë˜ëŠ” str.isdigit()ì¸ì§€ í™•ì¸)
        start, end = None, None
        
        if isinstance(start_val, int):
            start = start_val
        elif isinstance(start_val, str) and start_val.isdigit():
            start = int(start_val)
        
        if isinstance(end_val, int):
            end = end_val
        elif isinstance(end_val, str) and end_val.isdigit():
            end = int(end_val)

        # 5. start, endê°€ ì„±ê³µì ìœ¼ë¡œ ë³€í™˜ë˜ì—ˆëŠ”ì§€, ê·¸ë¦¬ê³  ìœ íš¨í•œ ë²”ìœ„ì¸ì§€ ìµœì¢… í™•ì¸
        # (ì˜ˆ: startê°€ endë³´ë‹¤ í¬ê±°ë‚˜, í…ìŠ¤íŠ¸ ê¸¸ì´ë¥¼ ë²—ì–´ë‚˜ëŠ” ê²½ìš° ë“±)
        if start is None or end is None or start >= end or end > len(text):
            continue


        # if label is None or start is None or end is None:
        #     # print("None")
        #     continue
        
        if start < len(text) and end <= len(text):
            char_tags[start] = f'B-{label}'
            for i in range(start + 1, end):
                char_tags[i] = f'I-{label}'

    encoding = tokenizer(text, return_offsets_mapping=True, add_special_tokens=False)
    offsets = encoding['offset_mapping']
    # print(f'offsets: {offsets}')
    # print(f'text: {text}')
    # print(f'char_tags: {char_tags}')
    token_tags = []
    for (start, end) in offsets:
        # offsetì´ (0, 0)ì¸ íŠ¹ìˆ˜ í† í°ì€ ì´ë¯¸ add_special_tokens=Falseë¡œ ì œì™¸ë¨
        # if start == end: continue
        token_tags.append(char_tags[start])
        
    return token_tags

    # 2. í† í°í™”í•˜ê³ , ê° í† í°ì— í•´ë‹¹í•˜ëŠ” BIO íƒœê·¸ ë§¤í•‘
    encoding = tokenizer(text, return_offsets_mapping=True)
    tokens = tokenizer.convert_ids_to_tokens(encoding['input_ids'])
    offsets = encoding['offset_mapping']
    
    token_tags = []
    for token, (start, end) in zip(tokens, offsets):
        # [CLS], [SEP], [PAD] ë“± íŠ¹ìˆ˜ í† í°ì€ ì œì™¸
        if start == end:
            continue
        
        # ê° í† í°ì˜ íƒœê·¸ëŠ” í•´ë‹¹ í† í°ì˜ ì‹œì‘ ë¬¸ìì˜ íƒœê·¸ë¥¼ ë”°ë¦„
        token_tags.append(char_tags[start])
        
    return token_tags
# --- 2. ë©”ì¸ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ---

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="Kanana-8B PII NER Fine-tuning Script with Caching")
    parser.add_argument('--mode', type=str, default='train', choices=['train', 'test'])
    parser.add_argument('--model_path', type=str, default="/mnt/data3/Korean_abstraction/python/coreference/models/kanana-1.5-2.1b-instruct-2505")
    parser.add_argument('--dataset_path', type=str, default='./datasets/pii_ner_3merged_08061_v3', help='Path to the raw dataset directory.')
    parser.add_argument('--processed_dataset_path', type=str, default='./processed_pii_dataset_kanana', help='Path to save/load the processed and split dataset.')
    parser.add_argument('--output_dir', type=str, default='./kanana_pii_finetuned')
    parser.add_argument('--lora_adapter_path', type=str, default='./kanana_pii_finetuned', help='í•™ìŠµëœ LoRA ì–´ëŒ‘í„° ê°€ì¤‘ì¹˜ê°€ ì €ì¥ëœ ê²½ë¡œ (ì˜ˆ: ./kanana_pii_finetuned)')
    
    
    args = parser.parse_args()

    # --- ê³µí†µ ì„¤ì •: í† í¬ë‚˜ì´ì € ë¡œë“œ ---
    # `add_eos_token=True`ëŠ” ì…ë ¥ì˜ ëì„ ëª…í™•íˆ ì•Œë ¤ì£¼ì–´ ëª¨ë¸ì˜ ë‹µë³€ ìƒì„±ì„ ë•ìŠµë‹ˆë‹¤.
    tokenizer = AutoTokenizer.from_pretrained(args.model_path, add_eos_token=True)
    tokenizer.pad_token = tokenizer.eos_token # PAD í† í°ì„ EOS í† í°ìœ¼ë¡œ ì„¤ì •
    args.dataset_path = './datasets/pii_ner_3ds_norm'
    # args.dataset_path = '/mnt/data3/Korean_abstraction/python/coreference/datasets/pii_ner_only_thunder_kluebert'
    # --- í•™ìŠµ ëª¨ë“œ ---
    if args.mode == 'train':
        print("--- íŒŒì¸íŠœë‹ ëª¨ë“œë¥¼ ì‹œì‘í•©ë‹ˆë‹¤ ---")
        
        # 1. ë°ì´í„°ì…‹ ë¡œë“œ ë° í”„ë¡¬í”„íŠ¸ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        if os.path.exists(args.processed_dataset_path):
            print(f"ì „ì²˜ë¦¬ëœ ë°ì´í„°ì…‹ì„ '{args.processed_dataset_path}'ì—ì„œ ë¡œë“œí•©ë‹ˆë‹¤...")
            train_dataset = load_from_disk(os.path.join(args.processed_dataset_path, 'train'))
            eval_dataset = load_from_disk(os.path.join(args.processed_dataset_path, 'validation'))
        else:
            print("ì „ì²˜ë¦¬ëœ ë°ì´í„°ì…‹ì´ ì—†ìŠµë‹ˆë‹¤. ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤...")

            # 2. í”„ë¡¬í”„íŠ¸ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            def formatting_prompts_func(example):
                instruction = "ì£¼ì–´ì§„ ë¬¸ì¥ì—ì„œ ëª¨ë“  ê°œì¸ ì‹ë³„ ì •ë³´(PII)ë¥¼ ì°¾ì•„ì„œ, ê° PIIì˜ ì¢…ë¥˜, ì‹œì‘ ì¸ë±ìŠ¤, ë ì¸ë±ìŠ¤ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ì¶”ì¶œí•˜ì„¸ìš”."
                input_text = example["sentence"]
                response_text = example["spans"]
                prompt = f"""### ì§€ì‹œ: {instruction}\n\n### ì…ë ¥:\n{input_text}\n\n### ë‹µë³€:\n{response_text}{tokenizer.eos_token}"""
                return {"text": prompt}
            
            # dataset = dataset.map(formatting_prompts_func, num_proc=os.cpu_count()) # ë³‘ë ¬ ì²˜ë¦¬ë¡œ ì†ë„ í–¥ìƒ
            full_train_dataset = load_from_disk(os.path.join(args.dataset_path, 'train'))
            train_dataset = full_train_dataset.map(formatting_prompts_func, remove_columns=full_train_dataset.column_names)
            train_dataset.save_to_disk(os.path.join(args.processed_dataset_path, 'train'))
            
            full_eval_dataset = load_from_disk(os.path.join(args.dataset_path, 'validation'))
            eval_dataset = full_eval_dataset.map(formatting_prompts_func, remove_columns=full_eval_dataset.column_names)
            eval_dataset.save_to_disk(os.path.join(args.processed_dataset_path, 'validation'))
            
            full_test_dataset = load_from_disk(os.path.join(args.dataset_path, 'test'))
            test_dataset = full_test_dataset.map(formatting_prompts_func, remove_columns=full_test_dataset.column_names)
            test_dataset.save_to_disk(os.path.join(args.processed_dataset_path, 'test'))
            
            # # 4. ë‹¤ìŒ ì‹¤í–‰ì„ ìœ„í•´ ë””ìŠ¤í¬ì— ì €ì¥
            # print(f"ì „ì²˜ë¦¬ëœ ë°ì´í„°ì…‹ì„ '{args.processed_dataset_path}' ê²½ë¡œì— ì €ì¥í•©ë‹ˆë‹¤...")
            # train_dataset.save_to_disk(os.path.join(args.processed_dataset_path, 'train'))
            # eval_dataset.save_to_disk(os.path.join(args.processed_dataset_path, 'validation'))


        def formatting_prompts_func(example):
            """ SFTTrainerì— ë§ëŠ” í˜•ì‹ìœ¼ë¡œ ë°ì´í„°ë¥¼ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜ """
            instruction = "ì£¼ì–´ì§„ ë¬¸ì¥ì—ì„œ ëª¨ë“  ê°œì¸ ì‹ë³„ ì •ë³´(PII)ë¥¼ ì°¾ì•„ì„œ, ê° PIIì˜ ì¢…ë¥˜, ì‹œì‘ ì¸ë±ìŠ¤, ë ì¸ë±ìŠ¤ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ì¶”ì¶œí•˜ì„¸ìš”."
            input_text = example["sentence"]
            response_text = example["spans"]

            # SFTTrainerëŠ” ì´ í˜•ì‹ì—ì„œ '### ë‹µë³€:' ë’·ë¶€ë¶„ì„ ì •ë‹µ(label)ìœ¼ë¡œ ì¸ì‹í•˜ì—¬ í•™ìŠµí•©ë‹ˆë‹¤.
            # EOS í† í°ì„ ë‹µë³€ ëì— ì¶”ê°€í•˜ì—¬ ëª¨ë¸ì´ ë‹µë³€ì„ ë§ˆì¹˜ëŠ” ì‹œì ì„ í•™ìŠµí•˜ë„ë¡ í•©ë‹ˆë‹¤.
            prompt = f"""### ì§€ì‹œ: {instruction}\n\n### ì…ë ¥:\n{input_text}\n\n### ë‹µë³€:\n{response_text}{tokenizer.eos_token}"""
            return {"text": prompt}
        # print(train_dataset[0]); quit()
        # dataset = dataset.map(formatting_prompts_func)
        # train_dataset = train_dataset.map(formatting_prompts_func)
        # eval_dataset = eval_dataset.map(formatting_prompts_func)

        # 2. ëª¨ë¸ ë¡œë“œ (8-bit ì–‘ìí™” ì ìš©)
        # BitsAndBytesConfigë¥¼ ì‚¬ìš©í•˜ì—¬ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¤„ì…ë‹ˆë‹¤.
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",          # 4ë¹„íŠ¸ ì–‘ìí™” íƒ€ì… (NF4ê°€ ì„±ëŠ¥ ì €í•˜ê°€ ê°€ì¥ ì ìŒ)
            bnb_4bit_compute_dtype=torch.bfloat16, # ê³„ì‚° ì‹œ ì‚¬ìš©í•  ë°ì´í„° íƒ€ì…
            bnb_4bit_use_double_quant=True,    # 2ì°¨ ì–‘ìí™”ë¡œ ë©”ëª¨ë¦¬ ì¶”ê°€ ì ˆì•½
        )
        
        device_map = {"": int(os.environ.get("LOCAL_RANK") or 0)}
        model = AutoModelForCausalLM.from_pretrained(
            args.model_path,
            quantization_config=bnb_config,
            torch_dtype=torch.bfloat16,
            device_map=device_map,
            # device_map="auto",
            trust_remote_code=True
        )
        
        # 3. LoRA ì„¤ì • (Parameter-Efficient Fine-Tuning)
        model = prepare_model_for_kbit_training(model)
        lora_config = LoraConfig(
            r=16,
            lora_alpha=32,
            target_modules=["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
            lora_dropout=0.05,
            bias="none",
            task_type="CAUSAL_LM"
        )
        peft_model = get_peft_model(model, lora_config)
        peft_model.print_trainable_parameters() # í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° ìˆ˜ ì¶œë ¥

        # 4. íŠ¸ë ˆì´ë„ˆ ì„¤ì •
        training_args = SFTConfig(
            output_dir=args.output_dir,
            num_train_epochs=2,                     # ì „ì²´ ë°ì´í„°ì…‹ì— ëŒ€í•œ í•™ìŠµ íšŸìˆ˜
            per_device_train_batch_size=16,          # ì¥ì¹˜ë‹¹ í•™ìŠµ ë°°ì¹˜ í¬ê¸°
            per_device_eval_batch_size=1,           # ì¥ì¹˜ë‹¹ ê²€ì¦ ë°°ì¹˜ í¬ê¸°
            gradient_accumulation_steps=4,          # ê·¸ë˜ë””ì–¸íŠ¸ ì¶•ì  ë‹¨ê³„ (ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ ìœ ìš©)
            learning_rate=2e-5,                     # í•™ìŠµë¥ 
            bf16=True,                              # bfloat16 ì‚¬ìš© (A100 ì´ìƒ GPUì—ì„œ íš¨ìœ¨ì )
            logging_strategy="steps",
            logging_steps=10,       
            eval_strategy="steps",
            eval_steps=6000,
            eval_accumulation_steps=3, 
            save_strategy="steps",
            save_steps=6000,                          # 50 ìŠ¤í…ë§ˆë‹¤ ì²´í¬í¬ì¸íŠ¸ ì €ì¥
            load_best_model_at_end=True,            # í•™ìŠµ ì¢…ë£Œ í›„ ìµœì  ëª¨ë¸ ë¡œë“œ
            metric_for_best_model="eval_f1_score",  # ìµœì  ëª¨ë¸ ì„ íƒ ê¸°ì¤€
            save_total_limit=4,                     # ìµœëŒ€ 4ê°œì˜ ì²´í¬í¬ì¸íŠ¸ë§Œ ì €ì¥
            report_to="none",                       # WandB ë“± ë¡œê¹… ë¹„í™œì„±í™”
            gradient_checkpointing=True,
            gradient_checkpointing_kwargs={"use_reentrant": False},
            max_length=1024,
            dataloader_num_workers=0,
        )
        
        trainer = SFTTrainer(
            model=peft_model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=eval_dataset,
            processing_class=tokenizer,
            compute_metrics=compute_metrics,
        )
        trainer = CustomStreamTrainer(
            model=peft_model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=eval_dataset,
            processing_class=tokenizer,
        )
        
        # 5. í•™ìŠµ ì‹œì‘
        print("íŒŒì¸íŠœë‹ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        trainer.train()
        print("í•™ìŠµ ì™„ë£Œ! ìµœì¢… ëª¨ë¸ì„ ì €ì¥í•©ë‹ˆë‹¤.")
        trainer.save_model(args.output_dir)

    elif args.mode == 'test':
        # 1. í† í¬ë‚˜ì´ì € ë¡œë“œ
        tokenizer = AutoTokenizer.from_pretrained(args.model_path)
        tokenizer.pad_token = tokenizer.eos_token

        # 2. ëª¨ë¸ ë¡œë”© (ë² ì´ìŠ¤ ëª¨ë¸ + í•™ìŠµëœ LoRA ì–´ëŒ‘í„° ë³‘í•©)
        print("ë² ì´ìŠ¤ ëª¨ë¸ ë° LoRA ì–´ëŒ‘í„° ë¡œë”© ì¤‘...")
        base_model = AutoModelForCausalLM.from_pretrained(
            args.model_path,
            torch_dtype=torch.bfloat16,
            device_map="auto",
            trust_remote_code=True
        )
        # ì €ì¥ëœ LoRA ê°€ì¤‘ì¹˜ë¥¼ ë² ì´ìŠ¤ ëª¨ë¸ì— ë³‘í•©
        model = PeftModel.from_pretrained(base_model, args.lora_adapter_path)
        model = model.merge_and_unload() # ì¶”ë¡  ì†ë„ í–¥ìƒì„ ìœ„í•´ ë³‘í•©
        model.eval()
        print("ëª¨ë¸ ë¡œë”© ë° ë³‘í•© ì™„ë£Œ.")

        # 3. í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ë¡œë“œ (ì „ì²˜ë¦¬ ì „ ì›ë³¸ ë°ì´í„°ì…‹)
        try:
            test_dataset = load_from_disk(os.path.join(args.dataset_path, 'test'))
            test_dataset = Subset(test_dataset, range(1500, 5000))
            print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ. ìƒ˜í”Œ ìˆ˜: {len(test_dataset)}")
        except FileNotFoundError:
            print(f"ì˜¤ë¥˜: '{args.dataset_path}/test' ê²½ë¡œì—ì„œ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            quit()
            

        # 4. ì „ì²´ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ì— ëŒ€í•´ ì¶”ë¡  ì‹¤í–‰
        all_true_entities = []
        all_pred_entities = []
        all_true_bio_tags_nested = [] 
        all_pred_bio_tags_nested = []
        all_true_bio_tags = []
        all_pred_bio_tags = []
        instruction = "ì£¼ì–´ì§„ ë¬¸ì¥ì—ì„œ ëª¨ë“  ê°œì¸ ì‹ë³„ ì •ë³´(PII)ë¥¼ ì°¾ì•„ì„œ, ê° PIIì˜ ì¢…ë¥˜, ì‹œì‘ ì¸ë±ìŠ¤, ë ì¸ë±ìŠ¤ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ì¶”ì¶œí•˜ì„¸ìš”."

        for example in tqdm(test_dataset, desc="Test ë°ì´í„°ì…‹ ì¶”ë¡  ì¤‘"):
            input_text = example["sentence"]
            true_bio_tags = []
            pred_bio_tags = []
            # ì •ë‹µ(spans)ì„ íŒŒì‹±í•˜ì—¬ ì •ë‹µ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
            try:
                spans_data = example.get("spans")
                if spans_data: # spans ë°ì´í„°ê°€ Noneì´ê±°ë‚˜ ë¹„ì–´ìˆì§€ ì•Šì€ ê²½ìš°ì—ë§Œ ì²˜ë¦¬
                    true_entities = json.loads(spans_data) if isinstance(spans_data, str) else spans_data
                    # print(f"Original true_entities: {true_entities}")
                    if true_entities: # íŒŒì‹± í›„ì—ë„ ì—”í‹°í‹°ê°€ ì‹¤ì œë¡œ ìˆëŠ” ê²½ìš°
                        true_bio_tags = convert_json_to_bio_tags(input_text, true_entities, tokenizer)
                    # print("true:", true_bio_tags)
                    # true_bio_tags = true_entities
                # # spans_dataì˜ íƒ€ì…ì´ ë¬¸ìì—´(str)ì¸ì§€ í™•ì¸
                # true_entities = json.loads(spans_data) if isinstance(spans_data, str) else spans_data
                
                # # all_true_entities.append(true_entities)
                
                # true_bio_tags = convert_json_to_bio_tags(input_text, true_entities, tokenizer)

            except (json.JSONDecodeError, TypeError): # TypeErrorë„ ì²˜ë¦¬
                print("Error parsing spans_data:", spans_data)
                continue
                # all_true_entities.append([]) # ì‹¤íŒ¨ ì‹œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ì¶”ê°€
                
            if true_bio_tags is None:
                # continue
                tokens = tokenizer(input_text, add_special_tokens=False)['input_ids']
                true_bio_tags = ['O'] * len(tokens)
            # print('true:',all_true_entities)
            # ì¶”ë¡ ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = f"""### ì§€ì‹œ: {instruction}\n\n### ì…ë ¥:\n{input_text}\n\n### ë‹µë³€:\n"""
            inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

            # ëª¨ë¸ ìƒì„±
            with torch.no_grad():
                outputs = model.generate(
                    **inputs, 
                    max_new_tokens=1024, 
                    eos_token_id=tokenizer.eos_token_id,
                    pad_token_id=tokenizer.pad_token_id
                )
            
            # ìƒì„±ëœ ê²°ê³¼ì—ì„œ í”„ë¡¬í”„íŠ¸ë¥¼ ì œì™¸í•œ ë‹µë³€ ë¶€ë¶„ë§Œ ë””ì½”ë”©
            response_text = tokenizer.decode(outputs[0][len(inputs["input_ids"][0]):], skip_special_tokens=True)
            parsed_json = extract_json_from_text(response_text)
            if parsed_json and isinstance(parsed_json, dict):
                pred_entities = parsed_json.get("PII", [])
                normalized_pred_entities = normalize_pii_types(pred_entities)
                pred_bio_tags = convert_json_to_bio_tags(input_text, normalized_pred_entities, tokenizer)
            else:
                try:
                    start_idx = response_text.find('```json')
                    end_idx = response_text.rfind('}')
                    
                    if start_idx != -1 and end_idx != -1 and start_idx < end_idx:
                        # ì •í™•í•œ JSON ë¬¸ìì—´ ë¶€ë¶„ë§Œ ì¶”ì¶œí•©ë‹ˆë‹¤.
                        pred_json_str = response_text[start_idx : end_idx + 1]
                        pred_json_str = pred_json_str[7:].strip()
                        # print(f'pred_json_str: >>>{pred_json_str}<<<')
                        # 2. ì¶”ì¶œëœ ë¬¸ìì—´ì„ JSONìœ¼ë¡œ íŒŒì‹±í•©ë‹ˆë‹¤.
                        parsed_json = json.loads(pred_json_str)
                        
                        # 3. {"PII": [...]} êµ¬ì¡°ì—ì„œ ì‹¤ì œ ì—”í‹°í‹° ë¦¬ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
                        # ë§Œì•½ 'PII' í‚¤ê°€ ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
                        pred_entities = parsed_json.get("PII", [])
                        # print(f'pred_entities: {pred_entities}')
                        normalized_pred_entities = normalize_pii_types(pred_entities)
                        # all_pred_entities.append(normalized_pred_entities)
                        pred_bio_tags = convert_json_to_bio_tags(input_text, pred_entities, tokenizer)
                    # else:
                    #     # JSON ê°ì²´ë¥¼ ì°¾ì§€ ëª»í•œ ê²½ìš°
                    #     all_pred_entities.append([])

                except (json.JSONDecodeError, KeyError):
                    # íŒŒì‹±ì— ì‹¤íŒ¨í•˜ê±°ë‚˜ "PII" í‚¤ê°€ ì—†ëŠ” ê²½ìš°
                    pred_bio_tags = ['O'] * len(true_bio_tags)
                    
                # all_pred_entities.append([])
            # print('pred:', all_pred_entities)
            # if  true_bio_tags is not None and len(true_bio_tags) == len(pred_bio_tags):
            if len(true_bio_tags) == len(pred_bio_tags):
                all_true_bio_tags.extend(true_bio_tags)
                all_pred_bio_tags.extend(pred_bio_tags)
                all_true_bio_tags_nested.append(true_bio_tags)
                all_pred_bio_tags_nested.append(pred_bio_tags)
        
        # # ê¸¸ì´ê°€ ë‹¤ë¥¼ ê²½ìš° í‰ê°€ì˜ ì¼ê´€ì„±ì„ ìœ„í•´ ì œì™¸ (ì¼ë°˜ì ìœ¼ë¡œ ë°œìƒí•˜ì§€ ì•ŠìŒ)
        # if len(true_bio_tags) == len(pred_bio_tags):
        #     all_pred_bio_tags.extend(pred_bio_tags)
        # else:
        #     # ê¸¸ì´ê°€ ë‹¤ë¥¼ ê²½ìš°, ì •ë‹µ íƒœê·¸ ë¦¬ìŠ¤íŠ¸ì—ì„œë„ í•´ë‹¹ ìƒ˜í”Œì„ ì œê±°
        #     del all_true_bio_tags[-1*len(true_bio_tags):]
        # # for ë°˜ë³µë¬¸ ì´í›„, F1 ê³„ì‚° ì „
        
        
        print("\nEntity-level ì¶”ë¡  ê²°ê³¼ë¥¼ 'entity_results.jsonl' íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤...")
        with open("b_entity_results.jsonl", "w", encoding="utf-8") as f:
            for true_ents, pred_ents in zip(all_true_bio_tags_nested, all_pred_bio_tags_nested):
                line = json.dumps({
                    "true_entities": true_ents,
                    "pred_entities": pred_ents
                }, ensure_ascii=False)
                f.write(line + "\n")
        print("ì €ì¥ ì™„ë£Œ.")

        # 4-2. Token-level ê²°ê³¼ë¥¼ JSONLë¡œ ì €ì¥
        print("Token-level ì¶”ë¡  ê²°ê³¼ë¥¼ 'bio_tag_results.jsonl' íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤...")
        with open("bio_token_tag_results.jsonl", "w", encoding="utf-8") as f:
            for true_tag, pred_tag in zip(all_true_bio_tags, all_pred_bio_tags):
                line = json.dumps({"true_token": true_tag, "pred_token": pred_tag}, ensure_ascii=False)
                f.write(line + "\n")
        print("ì €ì¥ ì™„ë£Œ.")
        
        # print(f"all_true_bio_tags_nested: {all_true_bio_tags_nested}")
        # print(f"all_pred_bio_tags_nested: {all_pred_bio_tags_nested}")
        # print(f"all_true_bio_tags: {all_true_bio_tags}")
        # print(f"all_pred_bio_tags: {all_pred_bio_tags}")
        
        if not all_true_bio_tags_nested:
            print("í‰ê°€í•  ìœ íš¨í•œ ìƒ˜í”Œì´ ì—†ìŠµë‹ˆë‹¤.")
            
        else:
            entity_micro_f1 = seq_f1_score(all_true_bio_tags_nested, all_pred_bio_tags_nested, average="micro", zero_division=0)
            print(f"\nEntity-level Micro F1-Score: {entity_micro_f1:.4f}")

            binary_true_nested = [
                ['O' if tag == 'O' else f"{tag.split('-')[0]}-PII" for tag in seq]
                for seq in all_true_bio_tags_nested
            ]
            binary_pred_nested = [
                ['O' if tag == 'O' else f"{tag.split('-')[0]}-PII" for tag in seq]
                for seq in all_pred_bio_tags_nested
            ]
            
            # ë‹¨ì¼ 'PII' í´ë˜ìŠ¤ì— ëŒ€í•œ F1 ì ìˆ˜ë§Œ ì¶”ì¶œ
            report = seq_classification_report(binary_true_nested, binary_pred_nested, output_dict=True, zero_division=0)
            binary_f1 = report.get('PII', {}).get('f1-score', 0.0)
            print(f"Entity-level Binary F1-Score (PII ì „ì²´): {binary_f1:.4f}")

            # # âœ¨ (ì°¸ê³ ) ê¸°ì¡´ì˜ í´ë˜ìŠ¤ë³„ ìƒì„¸ ë¦¬í¬íŠ¸ âœ¨
            # print("\n--- Classification Report (per PII type) ---")
            # full_report = seq_classification_report(all_true_bio_tags_nested, all_pred_bio_tags_nested, zero_division=0, digits=4)
            # print(full_report)        
        
            print("\n\n--- ìµœì¢… ì„±ëŠ¥ í‰ê°€ (Token-level) ---")
            # 1. Token-level Micro F1 Score ê³„ì‚°
            micro_f1 = sk_f1_score(all_true_bio_tags, all_pred_bio_tags, average='micro', zero_division=0)
            print(f"Token-level Micro F1: {micro_f1:.4f}")

            # 2. Token-level Binary F1 Score ê³„ì‚°
            # 'O' íƒœê·¸ëŠ” 0, ë‚˜ë¨¸ì§€(B-*, I-*)ëŠ” 1ë¡œ ë³€í™˜
            binary_true = ["O" if tag == 'O' else "PII" for tag in all_true_bio_tags]
            binary_pred = ["O" if tag == 'O' else "PII" for tag in all_pred_bio_tags]

            binary_f1 = sk_f1_score(binary_true, binary_pred, pos_label="PII", average='binary', zero_division=0)
            print(f"Token-level Binary F1: {binary_f1:.4f}")
            O_binary_f1 = sk_f1_score(binary_true, binary_pred, pos_label="O", average='binary', zero_division=0)
            print(f"O - Token-level Binary F1: {O_binary_f1:.4f}")

    elif args.mode == 'infer':
        print("--- í…ŒìŠ¤íŠ¸ ëª¨ë“œë¥¼ ì‹œì‘í•©ë‹ˆë‹¤ ---")

        # 1. ëª¨ë¸ ë¡œë”© (ë² ì´ìŠ¤ ëª¨ë¸ + í•™ìŠµëœ LoRA ì–´ëŒ‘í„°)
        print("ë² ì´ìŠ¤ ëª¨ë¸ ë° LoRA ì–´ëŒ‘í„° ë¡œë”© ì¤‘...")
        base_model = AutoModelForCausalLM.from_pretrained(
            args.model_path,
            torch_dtype=torch.bfloat16,
            device_map="auto",
            trust_remote_code=True
        )
        # ì €ì¥ëœ LoRA ê°€ì¤‘ì¹˜ë¥¼ ë² ì´ìŠ¤ ëª¨ë¸ì— ë³‘í•©
        model = PeftModel.from_pretrained(base_model, args.output_dir)
        model = model.merge_and_unload() # ì¶”ë¡  ì†ë„ í–¥ìƒì„ ìœ„í•´ ë³‘í•©
        model.eval()

        # 2. ì¶”ë¡ í•  í”„ë¡¬í”„íŠ¸ ì¤€ë¹„
        instruction = "ì£¼ì–´ì§„ ë¬¸ì¥ì—ì„œ ëª¨ë“  ê°œì¸ ì‹ë³„ ì •ë³´(PII)ë¥¼ ì°¾ì•„ì„œ, ê° PIIì˜ ì¢…ë¥˜, ì‹œì‘ ì¸ë±ìŠ¤, ë ì¸ë±ìŠ¤ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ì¶”ì¶œí•˜ì„¸ìš”."
        input_text = "ë‹´ë‹¹ìëŠ” í™ê¸¸ë™ì´ë©°, ì—°ë½ì²˜ëŠ” 010-1234-5678, ì´ë©”ì¼ ì£¼ì†ŒëŠ” gildong.hong@example.com ì…ë‹ˆë‹¤."
        
        prompt = f"""### ì§€ì‹œ: {instruction}\n\n### ì…ë ¥:\n{input_text}\n\n### ë‹µë³€:\n"""
        
        # 3. ëª¨ë¸ ì¶”ë¡  ì‹¤í–‰
        print("\n--- ì¶”ë¡  ì…ë ¥ ---")
        print(prompt)
        
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
        with torch.no_grad():
            outputs = model.generate(
                **inputs, 
                max_new_tokens=512, 
                eos_token_id=tokenizer.eos_token_id,
                pad_token_id=tokenizer.pad_token_id # pad_token_id ëª…ì‹œ
            )
        
        # ìƒì„±ëœ ê²°ê³¼ì—ì„œ í”„ë¡¬í”„íŠ¸ë¥¼ ì œì™¸í•œ ë‹µë³€ ë¶€ë¶„ë§Œ ë””ì½”ë”©
        response_text = tokenizer.decode(outputs[0][len(inputs["input_ids"][0]):], skip_special_tokens=True)

        print("\n--- ì¶”ë¡  ê²°ê³¼ ---")
        try:
            parsed_json = json.loads(response_text)
            print(json.dumps(parsed_json, indent=4, ensure_ascii=False))
        except json.JSONDecodeError:
            print("JSON íŒŒì‹±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì›ë³¸ ì¶œë ¥:")
            print(response_text)